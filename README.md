# INTEGRATION-PEROJECT-ETL-
This is ETL Project repository.

In this project, Iâ€™ve built an ETL pipeline using Python,SQL database & Snowflake. The goal was to extract data from flat files, clean and transform it into a useful format, and finally load it into an snowflake database. Here are the key steps:

Extract:
I retrieved data from sources -CSV files, or databases.
The original data formats varied (CSV, JSON, etc.), and I handled them accordingly.
Transform:
Data cleaning and transformation were essential. 
I used Python to preprocess and manipulate the data.(Using Faker Library)
This step involved tasks like handling Aggregated table data, MDM Tables, and applying business rules.
Load:
I chose an Snowflake database as the final destination.
The transformed data was loaded into appropriate tables within the database.
